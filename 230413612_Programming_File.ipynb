{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning Model for Financial Data Prediction\n",
        "\n",
        "#### Kabilan Mani\n",
        "#### 230413612\n",
        "#### ec230413612@qmul.ac.uk\n"
      ],
      "metadata": {
        "id": "v2TgFwJTQNm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Importing Essential Libraries\n"
      ],
      "metadata": {
        "id": "tIt5lZUxQUZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're bringing together all the tools and libraries that will empower our data analysis and model-building process. Each library has a specific role, from basic data manipulation with **numpy** and **pandas** to advanced machine learning with **xgboost** and **pytorch**."
      ],
      "metadata": {
        "id": "8dLsk0euQqpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M5DCwFrGQFDp"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for data manipulation, machine learning, and deep learning.\n",
        "# numpy and pandas are used for numerical operations and data handling.\n",
        "# sklearn provides tools for data preprocessing, model selection, and evaluation.\n",
        "# xgboost is a popular library for gradient boosting algorithms.\n",
        "# torch is used for building and training deep learning models.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from statsmodels.tsa.arima.model import ARIMA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Loading and Merging Datasets"
      ],
      "metadata": {
        "id": "VJ90Q61lRER7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're loading two key datasets, ensuring that their formats align, and then merging them based on the date. This merge gives us a powerful dataset where **financial indicators** and **stock performance** are side by side, laying the groundwork for our predictive models."
      ],
      "metadata": {
        "id": "xzY2uL0cRH1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load financial and stock data\n",
        "merged_financial_data = pd.read_csv('Merged_Financial_Data.csv')\n",
        "stock_data = pd.read_csv('TSLA_Quarterly_Data.csv')\n",
        "\n",
        "# Ensure column names are consistent\n",
        "merged_financial_data.columns = merged_financial_data.columns.astype(str)\n",
        "stock_data.columns = stock_data.columns.astype(str)\n",
        "\n",
        "# Convert 'Date' columns to datetime format\n",
        "merged_financial_data['Date'] = pd.to_datetime(merged_financial_data['Date'])\n",
        "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
        "\n",
        "# Merge datasets on 'Date'\n",
        "merged_df = pd.merge(merged_financial_data, stock_data, on='Date', how='inner')\n",
        "\n",
        "# Prepare features and target\n",
        "features = merged_df.drop(columns=['Date', 'Close_y'])\n",
        "target = merged_df['Close_y']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "lLUFfnLpRG6g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preparing Features and Scaling\n"
      ],
      "metadata": {
        "id": "M0bJHWlURbzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After merging the datasets, we prepare our feature matrix and target variable. Scaling the features ensures that each one contributes equally to the model, and splitting the data allows us to assess how well our model generalizes to unseen data."
      ],
      "metadata": {
        "id": "joUdXs8wRbpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the feature matrix (X) by dropping non-essential columns and setting the stock price as the target variable (y).\n",
        "features = merged_df.drop(columns=['Date', 'Close_y'])\n",
        "target = merged_df['Close_y']\n",
        "\n",
        "# Standardizing the feature matrix using StandardScaler.\n",
        "# This step is crucial for many machine learning algorithms, which perform better when features are on a similar scale.\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Splitting the dataset into training and testing sets to evaluate our model's performance.\n",
        "# We use 80% of the data for training and reserve 20% for testing.\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "hi6KgiI1RVwj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Handling Missing Data\n"
      ],
      "metadata": {
        "id": "RziLXfSGRjPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing data can be a common issue in real-world datasets. Here, we're using a straightforward imputation technique to fill in these gaps, ensuring that our model can work with a complete dataset."
      ],
      "metadata": {
        "id": "wTxDi77kRjKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Addressing any missing values in the dataset by imputing them with the mean of each feature.\n",
        "# This is a simple yet effective way to handle missing data without losing any rows from our dataset.\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)"
      ],
      "metadata": {
        "id": "J_ti-60QRhE1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Model"
      ],
      "metadata": {
        "id": "cKmHB9Fb8p84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1 ARIMA Model Training"
      ],
      "metadata": {
        "id": "whjQbwmNdorJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a comprehensive approach to building and evaluating an ARIMA model for time series forecasting. It first ensures the data is stationary, applies differencing if needed, and optionally tunes the ARIMA parameters using auto_arima.\n",
        "This workflow is typical in time series analysis and helps in creating reliable forecasting models."
      ],
      "metadata": {
        "id": "vcSWg2pa9zwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to test stationarity\n",
        "def test_stationarity(timeseries):\n",
        "    result = adfuller(timeseries)\n",
        "    print(f'ADF Statistic: {result[0]}')\n",
        "    print(f'p-value: {result[1]}')\n",
        "    for key, value in result[4].items():\n",
        "        print(f'Critical Values {key}: {value}')\n",
        "\n",
        "# Check if the time series is stationary\n",
        "test_stationarity(y_train)\n",
        "\n",
        "# Differencing if the series is not stationary\n",
        "if adfuller(y_train)[1] > 0.05:\n",
        "    y_train_diff = np.diff(y_train, n=1)\n",
        "    print(\"Applied first differencing\")\n",
        "else:\n",
        "    y_train_diff = y_train\n",
        "\n",
        "# Use auto_arima to find the best parameters (optional)\n",
        "try:\n",
        "    from pmdarima import auto_arima\n",
        "    auto_arima_model = auto_arima(y_train_diff, seasonal=False, trace=True)\n",
        "    print(auto_arima_model.summary())\n",
        "    p, d, q = auto_arima_model.order\n",
        "except ImportError:\n",
        "    # If pmdarima is not available, fall back to manual tuning\n",
        "    p, d, q = 5, 1, 0  # Default values to be adjusted\n",
        "\n",
        "# Train ARIMA model with tuned parameters\n",
        "arima_model = ARIMA(y_train_diff, order=(p, d, q))\n",
        "arima_result = arima_model.fit()\n",
        "\n",
        "# Forecast using ARIMA\n",
        "arima_forecast_diff = arima_result.forecast(steps=len(y_test))\n",
        "\n",
        "# Reverse differencing to obtain the forecast in the original scale\n",
        "arima_forecast = np.r_[y_train[-1], arima_forecast_diff].cumsum()[1:]\n",
        "\n",
        "# Evaluate ARIMA model\n",
        "arima_mse = mean_squared_error(y_test, arima_forecast)\n",
        "print(f'ARIMA Model MSE: {arima_mse:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGOs3SYydobC",
        "outputId": "aa7e3f5e-3f89-4ae8-85de-63bda77029c6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADF Statistic: -14.11899967915911\n",
            "p-value: 2.4437281550243927e-26\n",
            "Critical Values 1%: -6.045114\n",
            "Critical Values 5%: -3.9292800000000003\n",
            "Critical Values 10%: -2.98681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
            "  warn('Non-stationary starting autoregressive parameters'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Model MSE: 56256.5453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and transpose data for feature selection\n",
        "data_path = 'Final_Transposed_Financial_Data_with_Category.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df_transposed = df.set_index('category').T\n",
        "\n",
        "# Extract features and target\n",
        "target_column = 'Close'\n",
        "features = df_transposed.drop(columns=[target_column]).values\n",
        "target = df_transposed[target_column].values\n",
        "\n",
        "# Handle missing values\n",
        "features = pd.DataFrame(features).fillna(features.mean()).values\n",
        "target = np.nan_to_num(target)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n"
      ],
      "metadata": {
        "id": "UeXFMls63RcX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 XGBoost Model Training"
      ],
      "metadata": {
        "id": "QOI6TTQ8RtyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is a powerful tool for regression tasks. By training it on our data, we're creating a model that can learn the intricate patterns between financial metrics and stock prices, setting the stage for our predictions."
      ],
      "metadata": {
        "id": "_ZpfzRY9RtoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Check the shapes of the training data and labels\n",
        "print(f'Shape of X_train_imputed: {X_train_imputed.shape}')\n",
        "print(f'Shape of y_train: {y_train.shape}')\n",
        "print(f'Shape of X_test_imputed: {X_test_imputed.shape}')\n",
        "print(f'Shape of y_test: {y_test.shape}')\n",
        "\n",
        "# Ensure that X_train and y_train have the same number of rows\n",
        "min_train_len = min(X_train_imputed.shape[0], y_train.shape[0])\n",
        "X_train_imputed = X_train_imputed[:min_train_len]\n",
        "y_train = y_train[:min_train_len]\n",
        "\n",
        "# Ensure there are no missing values in y_train and y_test\n",
        "print(f'Missing values in y_train: {np.isnan(y_train).sum()}')\n",
        "print(f'Missing values in y_test: {np.isnan(y_test).sum()}')\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "try:\n",
        "    dtrain = xgb.DMatrix(X_train_imputed, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test_imputed, label=y_test)\n",
        "\n",
        "    # Define and train XGBoost model\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'max_depth': 7,\n",
        "        'learning_rate': 0.01,\n",
        "        'n_estimators': 100,\n",
        "        'verbosity': 1\n",
        "    }\n",
        "    bst = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "    # Make predictions\n",
        "    y_test_pred_xgb = bst.predict(dtest)\n",
        "\n",
        "    # Evaluate XGBoost model\n",
        "    xgb_mse = mean_squared_error(y_test, y_test_pred_xgb)\n",
        "    xgb_mae = mean_absolute_error(y_test, y_test_pred_xgb)\n",
        "    xgb_r2 = r2_score(y_test, y_test_pred_xgb)\n",
        "\n",
        "except xgb.core.XGBoostError as e:\n",
        "    print(f'XGBoost Error: {e}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SiohrWwRrwR",
        "outputId": "2ae13772-229f-4958-ba8a-b9e6828e032c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_imputed: (8, 92)\n",
            "Shape of y_train: (8,)\n",
            "Shape of X_test_imputed: (3, 92)\n",
            "Shape of y_test: (3,)\n",
            "Missing values in y_train: 0\n",
            "Missing values in y_test: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [20:15:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3 Hybrid Model (ARIMA + Neural Network) Training"
      ],
      "metadata": {
        "id": "o2_BNTAMfJnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This code implements a hybrid model combining the strengths of ARIMA and a neural network. The ARIMA model captures linear trends and patterns in the time series, while the neural network learns any remaining non-linear patterns from the residuals.\n",
        "1. By training on these residuals, the network helps improve the overall forecasting accuracy, making it a powerful approach for time series prediction tasks.\n",
        "2. The inclusion of early stopping and learning rate scheduling ensures that the model trains efficiently and avoids overfitting."
      ],
      "metadata": {
        "id": "SxSDtED7-ymn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Truncate y_train and ARIMA fitted values to have the same length\n",
        "min_len = min(len(y_train), len(arima_result.fittedvalues))\n",
        "y_train_truncated = y_train[:min_len]\n",
        "arima_fitted_truncated = arima_result.fittedvalues[:min_len]\n",
        "\n",
        "# Calculate residuals from ARIMA model\n",
        "arima_residuals = y_train_truncated - arima_fitted_truncated\n",
        "\n",
        "# Split data into training and validation sets for better evaluation\n",
        "residual_train, residual_val = train_test_split(arima_residuals, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare residual data for Neural Network\n",
        "residual_train_tensor = torch.tensor(residual_train, dtype=torch.float32).view(-1, 1)\n",
        "residual_val_tensor = torch.tensor(residual_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Define a more complex neural network model\n",
        "class ResidualNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResidualNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(residual_train_tensor.shape[1], 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.bn4 = nn.BatchNorm1d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = torch.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "nn_model = ResidualNN()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "# Training with early stopping, learning rate scheduling, and validation\n",
        "num_epochs = 100\n",
        "best_val_loss = float('inf')\n",
        "patience, trials = 10, 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    nn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = nn_model(residual_train_tensor)\n",
        "    loss = criterion(y_pred, residual_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    nn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = nn_model(residual_val_tensor)\n",
        "        val_loss = criterion(val_pred, residual_val_tensor)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        trials = 0\n",
        "    else:\n",
        "        trials += 1\n",
        "        if trials >= patience:\n",
        "            print(f\"Early stopping on epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
        "\n",
        "# After training, use the model for predictions or further evaluation\n"
      ],
      "metadata": {
        "id": "Y3DnhkjXfG3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9e6dc3-0b5d-4173-d70c-592980aec3c6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Training Loss: 18049.7324, Validation Loss: 4542.3193\n",
            "Epoch 2/100, Training Loss: 18021.4121, Validation Loss: 4537.5654\n",
            "Epoch 3/100, Training Loss: 17926.2441, Validation Loss: 4533.3760\n",
            "Epoch 4/100, Training Loss: 17964.0488, Validation Loss: 4527.6904\n",
            "Epoch 5/100, Training Loss: 17919.7734, Validation Loss: 4523.0640\n",
            "Epoch 6/100, Training Loss: 17864.1953, Validation Loss: 4516.7461\n",
            "Epoch 7/100, Training Loss: 17872.7480, Validation Loss: 4510.6758\n",
            "Epoch 8/100, Training Loss: 17889.1113, Validation Loss: 4502.5229\n",
            "Epoch 9/100, Training Loss: 17847.6270, Validation Loss: 4494.4131\n",
            "Epoch 10/100, Training Loss: 17843.9727, Validation Loss: 4484.7881\n",
            "Epoch 11/100, Training Loss: 17830.3828, Validation Loss: 4475.7832\n",
            "Epoch 12/100, Training Loss: 17846.5645, Validation Loss: 4466.7334\n",
            "Epoch 13/100, Training Loss: 17836.0176, Validation Loss: 4458.7559\n",
            "Epoch 14/100, Training Loss: 17811.8301, Validation Loss: 4448.6870\n",
            "Epoch 15/100, Training Loss: 17828.0566, Validation Loss: 4439.6006\n",
            "Epoch 16/100, Training Loss: 17802.0996, Validation Loss: 4431.2998\n",
            "Epoch 17/100, Training Loss: 17804.2793, Validation Loss: 4424.9160\n",
            "Epoch 18/100, Training Loss: 17795.6641, Validation Loss: 4417.6211\n",
            "Epoch 19/100, Training Loss: 17791.4746, Validation Loss: 4408.7549\n",
            "Epoch 20/100, Training Loss: 17780.8379, Validation Loss: 4400.7729\n",
            "Epoch 21/100, Training Loss: 17767.4219, Validation Loss: 4392.5186\n",
            "Epoch 22/100, Training Loss: 17761.1035, Validation Loss: 4381.8950\n",
            "Epoch 23/100, Training Loss: 17763.1777, Validation Loss: 4374.1519\n",
            "Epoch 24/100, Training Loss: 17714.4062, Validation Loss: 4368.3594\n",
            "Epoch 25/100, Training Loss: 17714.8398, Validation Loss: 4360.2896\n",
            "Epoch 26/100, Training Loss: 17708.1426, Validation Loss: 4351.6021\n",
            "Epoch 27/100, Training Loss: 17697.6367, Validation Loss: 4344.0938\n",
            "Epoch 28/100, Training Loss: 17696.1562, Validation Loss: 4332.3857\n",
            "Epoch 29/100, Training Loss: 17686.1602, Validation Loss: 4330.8818\n",
            "Epoch 30/100, Training Loss: 17680.6426, Validation Loss: 4325.3184\n",
            "Epoch 31/100, Training Loss: 17677.2070, Validation Loss: 4319.4185\n",
            "Epoch 32/100, Training Loss: 17660.7363, Validation Loss: 4314.8057\n",
            "Epoch 33/100, Training Loss: 17667.7227, Validation Loss: 4308.2705\n",
            "Epoch 34/100, Training Loss: 17658.7207, Validation Loss: 4301.4414\n",
            "Epoch 35/100, Training Loss: 17645.5664, Validation Loss: 4296.6348\n",
            "Epoch 36/100, Training Loss: 17644.8438, Validation Loss: 4290.7256\n",
            "Epoch 37/100, Training Loss: 17633.4668, Validation Loss: 4287.4619\n",
            "Epoch 38/100, Training Loss: 17627.0586, Validation Loss: 4283.7490\n",
            "Epoch 39/100, Training Loss: 17616.2012, Validation Loss: 4282.8315\n",
            "Epoch 40/100, Training Loss: 17606.5586, Validation Loss: 4278.1836\n",
            "Epoch 41/100, Training Loss: 17603.6621, Validation Loss: 4275.0293\n",
            "Epoch 42/100, Training Loss: 17599.1699, Validation Loss: 4272.4517\n",
            "Epoch 43/100, Training Loss: 17594.8887, Validation Loss: 4269.0654\n",
            "Epoch 44/100, Training Loss: 17584.6738, Validation Loss: 4263.2334\n",
            "Epoch 45/100, Training Loss: 17580.1406, Validation Loss: 4260.0195\n",
            "Epoch 46/100, Training Loss: 17569.1719, Validation Loss: 4258.5996\n",
            "Epoch 47/100, Training Loss: 17560.9434, Validation Loss: 4259.1191\n",
            "Epoch 48/100, Training Loss: 17554.4883, Validation Loss: 4254.3647\n",
            "Epoch 49/100, Training Loss: 17548.6094, Validation Loss: 4249.1685\n",
            "Epoch 50/100, Training Loss: 17538.7734, Validation Loss: 4240.3188\n",
            "Epoch 51/100, Training Loss: 17532.8105, Validation Loss: 4239.8516\n",
            "Epoch 52/100, Training Loss: 17527.6035, Validation Loss: 4234.4888\n",
            "Epoch 53/100, Training Loss: 17532.9316, Validation Loss: 4225.4106\n",
            "Epoch 54/100, Training Loss: 17516.5723, Validation Loss: 4219.7891\n",
            "Epoch 55/100, Training Loss: 17509.9785, Validation Loss: 4217.1753\n",
            "Epoch 56/100, Training Loss: 17505.4453, Validation Loss: 4210.1665\n",
            "Epoch 57/100, Training Loss: 17494.5078, Validation Loss: 4209.0220\n",
            "Epoch 58/100, Training Loss: 17488.9688, Validation Loss: 4207.5503\n",
            "Epoch 59/100, Training Loss: 17484.1465, Validation Loss: 4204.0933\n",
            "Epoch 60/100, Training Loss: 17479.6367, Validation Loss: 4201.3833\n",
            "Epoch 61/100, Training Loss: 17473.1934, Validation Loss: 4199.6357\n",
            "Epoch 62/100, Training Loss: 17467.4707, Validation Loss: 4200.4893\n",
            "Epoch 63/100, Training Loss: 17457.4453, Validation Loss: 4199.0142\n",
            "Epoch 64/100, Training Loss: 17451.9902, Validation Loss: 4190.6553\n",
            "Epoch 65/100, Training Loss: 17448.9121, Validation Loss: 4181.5718\n",
            "Epoch 66/100, Training Loss: 17440.9062, Validation Loss: 4179.6968\n",
            "Epoch 67/100, Training Loss: 17438.5879, Validation Loss: 4174.7041\n",
            "Epoch 68/100, Training Loss: 17434.2598, Validation Loss: 4169.0581\n",
            "Epoch 69/100, Training Loss: 17424.2012, Validation Loss: 4167.1528\n",
            "Epoch 70/100, Training Loss: 17416.5703, Validation Loss: 4163.9536\n",
            "Epoch 71/100, Training Loss: 17408.9121, Validation Loss: 4162.5068\n",
            "Epoch 72/100, Training Loss: 17402.1270, Validation Loss: 4161.4150\n",
            "Epoch 73/100, Training Loss: 17399.8105, Validation Loss: 4157.8711\n",
            "Epoch 74/100, Training Loss: 17392.5781, Validation Loss: 4152.4692\n",
            "Epoch 75/100, Training Loss: 17381.9688, Validation Loss: 4151.9966\n",
            "Epoch 76/100, Training Loss: 17375.0664, Validation Loss: 4153.5635\n",
            "Epoch 77/100, Training Loss: 17371.8281, Validation Loss: 4150.2661\n",
            "Epoch 78/100, Training Loss: 17362.6074, Validation Loss: 4148.8525\n",
            "Epoch 79/100, Training Loss: 17356.6465, Validation Loss: 4144.1704\n",
            "Epoch 80/100, Training Loss: 17350.7500, Validation Loss: 4137.4160\n",
            "Epoch 81/100, Training Loss: 17346.5801, Validation Loss: 4132.9873\n",
            "Epoch 82/100, Training Loss: 17336.2734, Validation Loss: 4136.7412\n",
            "Epoch 83/100, Training Loss: 17333.8438, Validation Loss: 4130.4170\n",
            "Epoch 84/100, Training Loss: 17323.0410, Validation Loss: 4127.8779\n",
            "Epoch 85/100, Training Loss: 17318.9375, Validation Loss: 4129.8394\n",
            "Epoch 86/100, Training Loss: 17314.8672, Validation Loss: 4123.8901\n",
            "Epoch 87/100, Training Loss: 17308.4980, Validation Loss: 4118.3101\n",
            "Epoch 88/100, Training Loss: 17302.6875, Validation Loss: 4120.5513\n",
            "Epoch 89/100, Training Loss: 17295.5176, Validation Loss: 4121.6206\n",
            "Epoch 90/100, Training Loss: 17287.4473, Validation Loss: 4114.0054\n",
            "Epoch 91/100, Training Loss: 17280.2422, Validation Loss: 4112.2603\n",
            "Epoch 92/100, Training Loss: 17276.1230, Validation Loss: 4105.6226\n",
            "Epoch 93/100, Training Loss: 17266.3926, Validation Loss: 4099.9058\n",
            "Epoch 94/100, Training Loss: 17260.9355, Validation Loss: 4095.8303\n",
            "Epoch 95/100, Training Loss: 17254.5078, Validation Loss: 4085.7905\n",
            "Epoch 96/100, Training Loss: 17246.3906, Validation Loss: 4084.0793\n",
            "Epoch 97/100, Training Loss: 17241.0723, Validation Loss: 4079.4072\n",
            "Epoch 98/100, Training Loss: 17237.2773, Validation Loss: 4079.2534\n",
            "Epoch 99/100, Training Loss: 17234.0332, Validation Loss: 4077.1682\n",
            "Epoch 100/100, Training Loss: 17223.0762, Validation Loss: 4075.3833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " After training the model, the below section forecasts future values and evaluates the modelâ€™s accuracy using the MSE metric."
      ],
      "metadata": {
        "id": "wRMlEy3k-AdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Evaluation of Models\n"
      ],
      "metadata": {
        "id": "Kas8l9nBSf5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code segment evaluates three different models on a test dataset:\n",
        "\n",
        "    **ARIMA Model**: A traditional time series model that captures linear patterns.\n",
        "    **XGBoost Model**: A powerful machine learning model that captures complex patterns and interactions.\n",
        "    **Hybrid Model**: A combination of ARIMA and a neural network, designed to capture both linear and non-linear patterns in the data.\n",
        "Each model's performance is measured using standard metrics like MSE, MAE, and R2 Score."
      ],
      "metadata": {
        "id": "EyfU2o-ySf2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Evaluate ARIMA model\n",
        "arima_mse = mean_squared_error(y_test, arima_forecast)\n",
        "arima_mae = mean_absolute_error(y_test, arima_forecast)\n",
        "arima_r2 = r2_score(y_test, arima_forecast)\n",
        "\n",
        "print(f'ARIMA Model MSE: {arima_mse:.4f}')\n",
        "print(f'ARIMA Model MAE: {arima_mae:.4f}')\n",
        "print(f'ARIMA Model R2 Score: {arima_r2:.4f}')\n",
        "\n",
        "# Convert arima_forecast to the correct shape if necessary\n",
        "arima_forecast_array = arima_forecast.reshape(-1, 1)\n",
        "\n",
        "#Evaluate XGBoost Model\n",
        "\n",
        "print(f'XGBoost Model MSE: {xgb_mse:.4f}')\n",
        "print(f'XGBoost Model MAE: {xgb_mae:.4f}')\n",
        "print(f'XGBoost Model R2 Score: {xgb_r2:.4f}')\n",
        "\n",
        "# Evaluate Hybrid model (Combine ARIMA forecast and neural network)\n",
        "nn_forecast = nn_model(torch.tensor(arima_forecast_array, dtype=torch.float32)).detach().numpy()\n",
        "hybrid_forecast = arima_forecast + nn_forecast.flatten()\n",
        "\n",
        "# Calculate metrics for the hybrid model\n",
        "hybrid_mse = mean_squared_error(y_test, hybrid_forecast)\n",
        "hybrid_mae = mean_absolute_error(y_test, hybrid_forecast)\n",
        "hybrid_r2 = r2_score(y_test, hybrid_forecast)\n",
        "\n",
        "print(f'Hybrid Model MSE: {hybrid_mse:.4f}')\n",
        "print(f'Hybrid Model MAE: {hybrid_mae:.4f}')\n",
        "print(f'Hybrid Model R2 Score: {hybrid_r2:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2er0v8tlSHam",
        "outputId": "88c69f6d-681e-4408-c67d-8ef614a58c02"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Model MSE: 56256.5453\n",
            "ARIMA Model MAE: 216.5778\n",
            "ARIMA Model R2 Score: -4.7823\n",
            "XGBoost Model MSE: 13287.9088\n",
            "XGBoost Model MAE: 100.3768\n",
            "XGBoost Model R2 Score: -0.3658\n",
            "Hybrid Model MSE: 58778.0403\n",
            "Hybrid Model MAE: 221.8783\n",
            "Hybrid Model R2 Score: -5.0415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Prediction of Next Value"
      ],
      "metadata": {
        "id": "fBPaEEAj5Wi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle ARIMA warnings by ensuring a proper index is set on the data\n",
        "y_test = y_test.reset_index(drop=True)  # Ensure index is properly set\n",
        "arima_forecast = arima_result.forecast(steps=1)[0]  # Correctly extract ARIMA forecast value\n",
        "print(f'Next value prediction using ARIMA: {arima_forecast:.4f}')\n",
        "\n",
        "# Predict the next value using the most recent data point from X_test\n",
        "next_input = X_test_imputed[-1].reshape(1, -1)  # Reshape to ensure it's in the right format\n",
        "next_dmatrix = xgb.DMatrix(next_input)\n",
        "next_value_pred_xgb = bst.predict(next_dmatrix)[0]  # Get the prediction for the next value\n",
        "\n",
        "print(f'Next value prediction using XGBoost: {next_value_pred_xgb:.4f}')\n",
        "\n",
        "# Hybrid Model: Combine ARIMA forecast and neural network residual prediction\n",
        "try:\n",
        "    next_residual = nn_model(torch.tensor([[arima_forecast]], dtype=torch.float32)).item()\n",
        "    next_value_hybrid = arima_forecast + next_residual\n",
        "    print(f'Next value prediction using Hybrid Model: {next_value_hybrid:.4f}')\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ezhgcGoSiNh",
        "outputId": "f3efddee-898b-4286-ecee-70ca61371dac"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next value prediction using ARIMA: 126.8539\n",
            "Next value prediction using XGBoost: 147.4598\n",
            "Next value prediction using Hybrid Model: 127.4968\n"
          ]
        }
      ]
    }
  ]
}